{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the kube-config file for the kubernetes cluster you want to deploy the configuration.\n",
    "\n",
    "ls /k8s/k8sConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp /k8s/k8sConfigs/morpheus.pmt-mk8s.conf $HOME/.kube/config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl get nodes\n",
    "kubectl get namespace\n",
    "kubectl get pods -A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fasttrack setup\n",
    "\n",
    "Assuming that the yml-files are edited to correspond to your actual setup, then the following steps will create the SimpliVity-Connector together with Prometheus and Grafana environment. \n",
    "\n",
    "If you want to take a look into the details and go step by step using this Jupyter notebook, then jump to chapter 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl apply -f namespace.yml             # create the namespace\n",
    "kubectl apply -f svtdemo.yml               # apply the SimpliVity connector configmap (in the following example the name svtdemo was used)\n",
    "kubectl apply -f svtconnector.yml          # create the SimpliVity connector pod\n",
    "kubectl apply -f prometheus.configmap.yml  # create the Prometheus configmap\n",
    "kubectl apply -f prometheus.pv-claim.yml   # create the persistent volume for the Prometheus database\n",
    "kubectl apply -f prometheus.yml            # create the Prometheus pod\n",
    "kubectl apply -f grafana.pv-claim.yml      # create the persistent volume for the Grafana database\n",
    "kubectl apply -f grafana.yml               # create the grafana pod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detailed Setup \n",
    "\n",
    "## 2.1 Create the namespace\n",
    "Create a namespace for your deployment. The example below creates the namespace svtprometheus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Namespace\n",
    "metadata:\n",
    "  name: svtprometheus\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create the SimpliVity - Prometheus connector POD\n",
    "\n",
    "### 2.2.1 Create the ConfigMap\n",
    "\n",
    "The connection and monitoring parameters are transferred to the HPE SimpliVity Prometheus connector as a Kubernetes ConfigMap. \n",
    "\n",
    "The ConfigMap can be created using th Python script: CreateConfigMap.py. \n",
    "\n",
    "Requirements:\n",
    "    - System with Python 3 and the following Python packages installed: \n",
    "      - Fernet\n",
    "      - getpass\n",
    "      - etree\n",
    "\n",
    "Run the script with the following command: \n",
    "\n",
    "#### python3 CreateConfigMap.py\n",
    "\n",
    "The script will ask for the following information:\n",
    "    - username               vCenter username (a user with readonly access rights is sufficient)\n",
    "    - password               vCenter password\n",
    "    - OVC/MVA IP address     IP address that the connector uses to connect to the federation\n",
    "    - name                   name of the yml-file (<name>.yml) and the configmap: <name>-xml that will be created\n",
    "    - port                   TCP Port that the connector uses to publish the counters.  \n",
    "    - K8s namespace          The Kubernetes namespace \n",
    "    \n",
    "It will create the Kubernetes yml-file (<name>.yml) that can be used to  create the Configmap <name>-xml \n",
    "\n",
    "The CreateConfigMap.py scripts sets for some of the SimpliVity connector parameters default values that can be edited in the file if needed:\n",
    "    \n",
    "    Parameter               Default value   Comment\n",
    "    - timerange             30              A range in seconds (the duration from the specified point in time)\n",
    "    - resolution            SECOND          The resolution (SECOND, MINUTE, HOUR, or DAY)\n",
    "    - monitoringinterval    30              connector cyle time (should be >= the time to process the captured data)\n",
    "    - monitor               fcn             performance data capture selector: f(ederation), c(luster), n(ode), v(irtual machine)\n",
    "    - cluster                               enter a clustername if you want to limit the data capture to a single cluster\n",
    "    - limit                 500             A positive integer that represents the maximum number of results to return\n",
    "    - offset                -1              A positive integer that directs the service to start returning the <offset value> instance, up to the limit. Every result will be collected if the offset is set to a negative value.  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the configmap (in the following example the name svtdemo was used)\n",
    "kubectl apply -f svtdemo.yml"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Example for the SimpliVity Prometheus Connector Configmap = do not use this example code.\n",
    "# Create your own configmap using the CreateConfigMap.py script! \n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: svtdemo-xml\n",
    "  namespace: test\n",
    "data:\n",
    "  svtconnector.key: |-\n",
    "    urAmRTcW4ZAYEu072heHhVy=\n",
    "  svtconnector.xml: |-\n",
    "    <data>\n",
    "      <username>SvtCollector@vsphere.local</username>\n",
    "      <user>gAAAAABevov1_SDs9BOwuE1qIaMMuK8r_cPWhW9g_2AJoimwWo-4=</user>\n",
    "      <password>gAAAAABevov1qE1aqFc1K0fP0ax3gbo4zGYNG_ANQ==</password>\n",
    "      <ovc>10.80.40.98</ovc>\n",
    "      <timerange>30</timerange>\n",
    "      <resolution>SECOND</resolution>\n",
    "      <monitoringintervall>30</monitoringintervall>\n",
    "      <logfile>svtdemo.log</logfile>\n",
    "      <port>9091</port>\n",
    "      <monitor>fcn</monitor>\n",
    "      <cluster></cluster>\n",
    "      <limit>100</limit>\n",
    "      <offset>-1</offset>\n",
    "    </data>  \n",
    "EOF  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Create the SimpliVity connector POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: svtconnector\n",
    "  namespace: svtprometheus      \n",
    "  labels:\n",
    "    app: svtconnector\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: svtconnector\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: svtconnector\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: svtconnector\n",
    "          image: tb1378/svtconk8s\n",
    "          command: [\"/usr/bin/python3\"]\n",
    "          args: [\"/opt/svt/svtpromconnector.py\"]\n",
    "          volumeMounts:\n",
    "            - name: svtconnectorxml\n",
    "              mountPath: /opt/svt/data\n",
    "      volumes:\n",
    "        - name: svtconnectorxml\n",
    "          configMap:\n",
    "            name: svtdemo-xml   # the correct name of the configmap needs to be added here. \n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: svtconnector-service\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    app: svtconnector\n",
    "  ports:\n",
    "    - port: 9091               # The Port of that the SimpliVity connector uses\n",
    "      targetPort: 9091\n",
    "      protocol: TCP\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.3. Deploy PrometheusÂ¶\n",
    "\n",
    "The Prometheus POD will be deployed with service accounts, a Config Map for the input parameters and a persistent volume for the Prometheus database. We will use here three yml-files (prometheus.configmap.yml, prometheus.pv-claim.yml and prometheus.yml) instead of a single one, in order to make the process a bit easier to understand.\n",
    "\n",
    "### 2.3.1 Create the Prometheus Configmap\n",
    "\n",
    "The Config Map will be used to define the Prometheus monitoring jobs. Each SimpliVity connector will require a separate Prometheus monitoring job, where each job will have the following entries:\n",
    "\n",
    "- job_name: 'simplivity-demo'\n",
    "  static_configs:\n",
    "  - targets: ['svtconnector:9091'] \n",
    "  honor_timestamps: true\n",
    "  scrape_interval: 30s\n",
    "  scrape_timeout: 10s\n",
    "  metrics_path: /metrics\n",
    "  scheme: htt\n",
    "The above job, will scrape every 30 seconds the data from the target svtconnector:9091, where svtconnector is the name and 9091 is the port that is used by the SimpliVity Prometheus connector container (as it was deployed in 3.) The target name, port and scrape_intervall should be adjusted to the actual values of the actual implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    " name: prometheus-config\n",
    " namespace: svtprometheus\n",
    "data:\n",
    " prometheus.yml: |\n",
    "  global:\n",
    "    scrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\n",
    "    evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n",
    "  alerting:\n",
    "    alertmanagers:\n",
    "    - static_configs:\n",
    "      - targets:\n",
    "        # - alertmanager:9093\n",
    "\n",
    "  # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.\n",
    "  rule_files:\n",
    "    # - \"first_rules.yml\"\n",
    "    # - \"second_rules.yml\"\n",
    "\n",
    "  # A scrape configuration containing exactly one endpoint to scrape:\n",
    "  # Here it's Prometheus itself.\n",
    "  scrape_configs:\n",
    "    # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.\n",
    "    - job_name: 'prometheus'\n",
    "\n",
    "      # metrics_path defaults to '/metrics'\n",
    "      # scheme defaults to 'http'.\n",
    "\n",
    "      static_configs:\n",
    "      - targets: ['localhost:9090']\n",
    "      \n",
    "    - job_name: 'simplivity-demo'\n",
    "      static_configs:\n",
    "      - targets: ['svtdemo:9091']\n",
    "      honor_timestamps: true\n",
    "      scrape_interval: 30s\n",
    "      scrape_timeout: 10s\n",
    "      metrics_path: /metrics\n",
    "      scheme: http\n",
    "    - job_name: 'ctcinfrastructure'\n",
    "      static_configs:\n",
    "      - targets: ['svtinfra:9091']\n",
    "      honor_timestamps: true\n",
    "      scrape_interval: 30s\n",
    "      scrape_timeout: 10s\n",
    "      metrics_path: /metrics\n",
    "      scheme: http\n",
    "\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Create the persistent volume claim for the database\n",
    "\n",
    "A persistent volume claim is used for the Prometheus database content.\n",
    "The following example uses a NFS share for the persistent volume of the Prometheus database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: prom-db-volume\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  persistentVolumeReclaimPolicy: Recycle\n",
    "  volumeMode: Filesystem\n",
    "  storageClassName: \"nfs\"\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  nfs:\n",
    "    server: 10.1.41.12\n",
    "    path: /k8s/pvc/prometheus\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: prometheus-pv-claim\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  storageClassName: \"nfs\"\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Create the Prometheus POD and ServiceÂ¶\n",
    "The Prometheus POD is created with the Prometheus Config Map and the persistent volume claim, that were defined in step 2.3.1 and 2.3.2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: ServiceAccount\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRole\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "rules:\n",
    "- apiGroups: [\"\"]\n",
    "  resources:\n",
    "  - nodes\n",
    "  - nodes/proxy\n",
    "  - services\n",
    "  - endpoints\n",
    "  - pods\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- apiGroups:\n",
    "  - extensions\n",
    "  resources:\n",
    "  - ingresses\n",
    "  verbs: [\"get\", \"list\", \"watch\"]\n",
    "- nonResourceURLs: [\"/metrics\"]\n",
    "  verbs: [\"get\"]\n",
    "---\n",
    "apiVersion: rbac.authorization.k8s.io/v1\n",
    "kind: ClusterRoleBinding\n",
    "metadata:\n",
    "  name: prometheus\n",
    "roleRef:\n",
    "  apiGroup: rbac.authorization.k8s.io\n",
    "  kind: ClusterRole\n",
    "  name: prometheus\n",
    "subjects:\n",
    "- kind: ServiceAccount\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: grafana\n",
    "      tier: backend\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: grafana\n",
    "        tier: backend\n",
    "    spec:\n",
    "      serviceAccountName: prometheus\n",
    "      containers:\n",
    "        - name: prometheus\n",
    "          image: prom/prometheus\n",
    "          args:\n",
    "            - '--storage.tsdb.path=/prometheus'\n",
    "            - '--storage.tsdb.no-lockfile'\n",
    "            - '--config.file=/etc/prometheus/prometheus.yml'\n",
    "          ports:\n",
    "            - containerPort: 9090\n",
    "              name: prometheus-port\n",
    "          volumeMounts:\n",
    "            - name: config-prometheus\n",
    "              mountPath: \"/etc/prometheus/\"\n",
    "            - name: prometheus-db\n",
    "              mountPath: \"/prometheus\"\n",
    "      volumes:     \n",
    "        - name: config-prometheus\n",
    "          configMap:\n",
    "            name: prometheus-config\n",
    "        - name: prometheus-db\n",
    "          persistentVolumeClaim:\n",
    "            claimName: prometheus-pv-claim\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: prometheus\n",
    "  namespace: svtprometheus\n",
    "  labels: \n",
    "     hpecp.hpe.com/hpecp-internal-gateway: \"true\"\n",
    "spec:\n",
    "  selector:\n",
    "    app: grafana\n",
    "    tier: backend\n",
    "  ports:\n",
    "    - port: 9090\n",
    "      targetPort: 9090\n",
    "      protocol: TCP\n",
    "  type: LoadBalancer\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Deploy Grafana\n",
    "\n",
    "### 2.4.1 Create the persistent volume for the Grafana Database\n",
    "The following example shows a persistent volume provided by a NFS server (10.1.41.12) on the path /k8s/pvc/grafana.\n",
    "The actual persistent volume the will be used, need to be adjusted to reflect your environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: PersistentVolume\n",
    "metadata:\n",
    "  name: grafana-db-volume\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  persistentVolumeReclaimPolicy: Retain # Recycle, Delete\n",
    "  volumeMode: Filesystem\n",
    "  storageClassName: \"nfs\"\n",
    "  capacity:\n",
    "    storage: 1Gi\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  nfs:                        \n",
    "    server: 10.1.41.12        \n",
    "    path: /k8s/pvc/grafana\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: grafana-pv-claim\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  storageClassName: \"nfs\"\n",
    "  accessModes:\n",
    "    - ReadWriteMany\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 1Gi\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 Create the Grafana POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << 'EOF' | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: grafana-service\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    app: grafana\n",
    "    tier: frontend\n",
    "  ports:\n",
    "    - port: 3000\n",
    "      targetPort: 3000\n",
    "      protocol: TCP\n",
    "  type: LoadBalancer\n",
    "---\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: grafana\n",
    "  namespace: svtprometheus\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: grafana\n",
    "      tier: frontend\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: grafana\n",
    "        tier: frontend\n",
    "    spec:     \n",
    "      containers:\n",
    "        - name: grafana\n",
    "          image: grafana/grafana\n",
    "          ports:\n",
    "            - containerPort: 3000\n",
    "              name: grafana-port\n",
    "              \n",
    "          volumeMounts:\n",
    "            - name: grafana-persistent-storage\n",
    "              mountPath: \"/var/lib/grafana\"\n",
    "          env:  \n",
    "            - name: GF_SMTP_ENABLED\n",
    "              value: \"yes\"\n",
    "            - name: GF_SMTP_HOST\n",
    "              value: \"smtp.hpe.com:25\"\n",
    "            - name: GF_SMTP_FROM_NAME\n",
    "              value: \"Beha, Thomas\"\n",
    "            - name: GF_SMTP_FROM_ADDRESS\n",
    "              value: \"thomas.beha@hpe.com\"\n",
    "            - name: GF_SMTP_SKIP_VERIFY\n",
    "              value: \"true\"\n",
    "            - name: GF_SERVER_HTTP_PORT\n",
    "              value: \"3000\"\n",
    "            - name: GF_INSTALL_PLUGINS\n",
    "              value: \"grafana-kubernetes-app\"\n",
    "#           - name: GF_INSTALL_PLUGINS\n",
    "#             value: \"ryantxu-ajax-panel\"\n",
    "#           - name: GF_PANELS_DISABLE_SANITIZE_HTML\n",
    "#             value: \"true\"\n",
    "#          - name: GF_AUTH_DISABLE_LOGIN_FORM\n",
    "#            value: \"true\"\n",
    "#          - name: GF_AUTH_ANONYMOUS_ENABLED\n",
    "#            value: 'true'\n",
    "      volumes:\n",
    "        - name: grafana-persistent-storage\n",
    "          persistentVolumeClaim:\n",
    "            claimName: grafana-pv-claim  \n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Check that everything is up and running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                READY   STATUS             RESTARTS   AGE\n",
      "pod/dhci-hpe-array-exporter-74bccf7d76-fs9mx        1/1     Running            0          88d\n",
      "pod/grafana-7c6ddfb4b7-9bjh5                        1/1     Running            0          121d\n",
      "pod/nimblefc-hpe-array-exporter-c467477bb-z9twt     1/1     Running            0          88d\n",
      "pod/primera630-hpe-array-exporter-544d64556-vd9q5   1/1     Running            0          88d\n",
      "pod/primera650-hpe-array-exporter-6dfcd6688-s9j6s   1/1     Running            0          89d\n",
      "pod/prometheus-647db98d77-p7vlf                     1/1     Running            0          88d\n",
      "pod/svtdemo-854bd96c8c-2n8lh                        0/1     CrashLoopBackOff   2075       98d\n",
      "pod/svtinfra-6c5c875bc9-fbz7r                       1/1     Running            17         121d\n",
      "\n",
      "NAME                                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE\n",
      "service/dhci-hpe-array-exporter         ClusterIP   10.98.60.160     <none>        9090/TCP         88d\n",
      "service/grafana-service                 NodePort    10.99.178.61     <none>        3000:31200/TCP   121d\n",
      "service/nimblefc-hpe-array-exporter     ClusterIP   10.99.144.131    <none>        9090/TCP         88d\n",
      "service/primera630-hpe-array-exporter   NodePort    10.103.139.106   <none>        9090:32110/TCP   88d\n",
      "service/primera650-hpe-array-exporter   NodePort    10.104.179.2     <none>        9090:30467/TCP   89d\n",
      "service/prometheus                      NodePort    10.96.144.183    <none>        9090:30585/TCP   121d\n",
      "service/svtdemo                         NodePort    10.108.124.136   <none>        9091:31390/TCP   121d\n",
      "service/svtinfra                        NodePort    10.107.29.214    <none>        9091:30441/TCP   121d\n",
      "\n",
      "NAME                              DATA   AGE\n",
      "configmap/kube-root-ca.crt        1      122d\n",
      "configmap/prometheus-config       1      121d\n",
      "configmap/svtdemo-xml             2      122d\n",
      "configmap/svtinfrastructure-xml   2      122d\n",
      "\n",
      "NAME                                 CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                               STORAGECLASS   REASON   AGE\n",
      "persistentvolume/grafana-db-volume   1Gi        RWX            Retain           Bound    svtprometheus/grafana-pv-claim      nfs                     121d\n",
      "persistentvolume/prom-db-volume      1Gi        RWX            Retain           Bound    svtprometheus/prometheus-pv-claim   nfs                     121d\n"
     ]
    }
   ],
   "source": [
    "kubectl -n svtprometheus get pods,svc,configmaps,pv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Delete everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kubectl delete -f grafana.yml       \n",
    "kubectl delete -f grafana.pv-claim.yml\n",
    "kubectl delete -f prometheus.yml\n",
    "kubectl delete -f prometheus.pv-claim.yml\n",
    "kubectl delete -f prometheus.configmap.yml\n",
    "kubectl delete -f svtconnector.yml \n",
    "kubectl delete -f svtdemo.yml \n",
    "kubectl delete -f namespace.yml "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
